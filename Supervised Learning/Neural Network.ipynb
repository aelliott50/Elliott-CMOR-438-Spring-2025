{
 "cells": [
  {
   "cell_type": "code",
   "id": "f59c5a7a3528467b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T10:35:38.019944Z",
     "start_time": "2025-05-06T10:35:37.961683Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "sns.set_theme()"
   ],
   "outputs": [],
   "execution_count": 135
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T00:56:41.707627Z",
     "start_time": "2025-05-03T00:56:41.704165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize weight matrices and bias vectors\n",
    "def initialize_weights(layers=(784, 60, 60, 10)):\n",
    "\n",
    "    W = [[0.0]]\n",
    "    B = [[0.0]]\n",
    "    for i in range(1, len(layers)):\n",
    "\n",
    "        w_temp = np.random.randn(layers[i], layers[i-1])*np.sqrt(2/layers[i-1])\n",
    "        b_temp = np.random.randn(layers[i], 1)*np.sqrt(2/layers[i-1])\n",
    "\n",
    "        W.append(w_temp)\n",
    "        B.append(b_temp)\n",
    "    return W, B"
   ],
   "id": "3cfd48ffa846aa08",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T00:56:43.724427Z",
     "start_time": "2025-05-03T00:56:43.721479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the sigmoid activation function, its derivative and mean squared error\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def mse(a, y):\n",
    "    return 0.5*sum((a-y)**2)"
   ],
   "id": "1911dc220eb3eb0c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_kmnist_numpy():\n",
    "    # Load KMNIST in supervised format\n",
    "    train_ds, test_ds = tfds.load('kmnist', split=['train', 'test'], as_supervised=True)\n",
    "\n",
    "    # Convert dataset to NumPy arrays\n",
    "    def to_numpy(ds):\n",
    "        images = []\n",
    "        labels = []\n",
    "        for image, label in tfds.as_numpy(ds):\n",
    "            image = image.astype(np.float32) / 255.0  # Normalize\n",
    "            image = np.squeeze(image)  # Shape: (28, 28)\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "        return np.array(images), np.array(labels)\n",
    "\n",
    "    X_train, y_train = to_numpy(train_ds)\n",
    "    X_test, y_test = to_numpy(test_ds)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Example usage\n",
    "X_train, y_train, X_test, y_test = load_kmnist_numpy()\n",
    "print(f\"Train set: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}, {y_test.shape}\")"
   ],
   "id": "8c76d8f89c3722b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T18:17:16.337351Z",
     "start_time": "2025-05-05T18:17:16.333062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Mapping each of the ten japanese characters to digits 0-10\n",
    "\"\"\"\"\n",
    "0: お\n",
    "1: き\n",
    "2: す\n",
    "3: つ\n",
    "4: な\n",
    "5: は\n",
    "6: ま\n",
    "7: や\n",
    "8: れ\n",
    "9: を\n",
    "\"\"\"\n",
    "hiragana_mapping = {\n",
    "    0: 'お',\n",
    "    1: 'き',\n",
    "    2: 'す',\n",
    "    3: 'つ',\n",
    "    4: 'な',\n",
    "    5: 'は',\n",
    "    6: 'ま',\n",
    "    7: 'や',\n",
    "    8: 'れ',\n",
    "    9: 'を'\n",
    "}"
   ],
   "id": "ad7a422b279080c9",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T18:17:19.473343Z",
     "start_time": "2025-05-05T18:17:19.397394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.imshow(X_train[0])\n",
    "plt.show()\n",
    "print(\"Character: \" + str(hiragana_mapping[y_train[0]]))"
   ],
   "id": "90de6c319245f70e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character: を\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Flatten the training images into column vectors.\n",
    "flat_train_X = []\n",
    "# One hot encode the training labels\n",
    "onehot_train_y = []\n",
    "\n",
    "for x, y in zip(X_train, y_train):\n",
    "    flat_train_X.append(x.flatten().reshape(784, 1))\n",
    "    temp_vec = np.zeros((10, 1))\n",
    "    temp_vec[y][0] = 1.0\n",
    "    onehot_train_y.append(temp_vec)\n",
    "\n",
    "\n",
    "# Do the same for the testing data\n",
    "flat_test_X = []\n",
    "onehot_test_y = []\n",
    "\n",
    "for x, y in zip(X_test, y_test):\n",
    "    flat_test_X.append(x.flatten().reshape(784, 1))\n",
    "    temp_vec = np.zeros((10, 1))\n",
    "    temp_vec[y] = 1.0\n",
    "    onehot_test_y.append(temp_vec)\n"
   ],
   "id": "85ebb0a5201f359",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T21:03:19.424361Z",
     "start_time": "2025-05-05T21:03:19.411500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize weights\n",
    "W, B = initialize_weights()\n",
    "\n",
    "def forward_pass(W, B, xi, predict_vector = False):\n",
    "    Z = [[0.0]]\n",
    "    A = [xi]\n",
    "    L = len(W) - 1\n",
    "    for i in range(1, L + 1):\n",
    "        z = W[i] @ A[i-1] + B[i]\n",
    "        Z.append(z)\n",
    "\n",
    "        a = sigmoid(z)\n",
    "        A.append(a)\n",
    "\n",
    "    if not predict_vector:\n",
    "        return Z, A\n",
    "    else:\n",
    "        return A[-1]\n",
    "\n",
    "def predict(W, B, xi):\n",
    "    _, A = forward_pass(W, B, xi)\n",
    "    return np.argmax(A[-1])"
   ],
   "id": "2531d04f2eae99aa",
   "outputs": [],
   "execution_count": 118
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T16:20:09.883967Z",
     "start_time": "2025-05-05T16:20:09.879624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def random_experiment(W, B, data_features, data_labels):\n",
    "    i = np.random.randint(len(data_features))\n",
    "    print(f\"Actual label: {np.argmax(data_labels[i])}\")\n",
    "    print(f\"Predicted label: {predict(W, B, data_features[i])}\")\n",
    "\n",
    "\n",
    "def MSE(W, B, X, y):\n",
    "    cost = 0.0\n",
    "    m = 0\n",
    "    for xi, yi in zip(X, y):\n",
    "        a = forward_pass(W, B, xi, predict_vector = True)\n",
    "        cost += mse(a, yi)\n",
    "        m+=1\n",
    "    return cost/m"
   ],
   "id": "ec848e2be546c8a7",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T21:21:30.497109Z",
     "start_time": "2025-05-05T21:21:30.487239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DenseNetwork:\n",
    "    def __init__(self, layers=(784, 60, 60, 10)):\n",
    "        self.layers = layers\n",
    "        self.W, self.B = initialize_weights(layers)\n",
    "        self.errors = []\n",
    "\n",
    "    def train(self, X_train, y_train, alpha=0.05, epochs=10):\n",
    "        # Print the initial mean squared error\n",
    "        self.errors = [MSE(self.W, self.B, X_train, y_train)]\n",
    "        print(f\"Starting Cost = {self.errors[0]}\")\n",
    "\n",
    "        # Find your sample size\n",
    "        sample_size = len(X_train)\n",
    "\n",
    "        # Find the number of non-input layers.\n",
    "        L = len(self.layers) - 1\n",
    "\n",
    "        # For each epoch perform stochastic gradient descent.\n",
    "        for k in range(epochs):\n",
    "            # Loop over each (xi, yi) training pair of data.\n",
    "            for xi, yi in zip(X_train, y_train):\n",
    "                # Use the forward pass function defined before\n",
    "                # and find the preactivation and postactivation values.\n",
    "                Z, A = forward_pass(self.W, self.B, xi)\n",
    "\n",
    "                # Store the errors in a dictionary for clear interpretation\n",
    "                # of computation of these values.\n",
    "                deltas = dict()\n",
    "\n",
    "                # Compute the output error\n",
    "                output_error = (A[L] - yi)*sigmoid_prime(Z[L])\n",
    "                deltas[L] = output_error\n",
    "\n",
    "                # Loop from L-1 to 1. Recall the right entry of the range function\n",
    "                # is non-inclusive.\n",
    "                for i in range(L-1, 0, -1):\n",
    "                    # Compute the node errors at each hidden layer\n",
    "                    deltas[i] = (self.W[i+1].T @ deltas[i+1])*sigmoid_prime(Z[i])\n",
    "\n",
    "                # Loop over each hidden layer and the output layer to perform gradient\n",
    "                # descent.\n",
    "                for i in range(1, L+1):\n",
    "                    self.W[i] -= alpha*deltas[i] @ A[i-1].T\n",
    "                    self.B[i] -= alpha*deltas[i]\n",
    "\n",
    "            # Show the user the cost over all training examples\n",
    "            self.errors.append(MSE(self.W, self.B, X_train, y_train))\n",
    "            print(f\"{k + 1}-Epoch Cost = {self.errors[-1]}\")\n",
    "\n",
    "\n",
    "    def predict(self, xi):\n",
    "        depth = len(self.layers)\n",
    "        _, A = forward_pass(self.W, self.B, xi)\n",
    "        return np.argmax(A[-1])"
   ],
   "id": "7c7c684c33baed5a",
   "outputs": [],
   "execution_count": 125
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "net = DenseNetwork()\n",
    "# MSE pre-training\n",
    "print(f\"MSE(net.W, net.B, flat_train_X, onehot_train_y) = {MSE(net.W, net.B, flat_train_X, onehot_train_y)} \\n\")\n",
    "net.train(flat_train_X, onehot_train_y, alpha=0.05, epochs=100)"
   ],
   "id": "e48367632967b7aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualize the mean squared error over the training process\n",
    "plt.figure(figsize = (10, 8))\n",
    "epochs = range(len(net.errors))\n",
    "plt.plot(epochs, net.errors, marker = \"o\")\n",
    "plt.xticks(epochs[::10])\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title(\"Network MSE During Training\", fontsize = 16)\n",
    "plt.show()"
   ],
   "id": "782f080edde45017",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example prediction\n",
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted character is: {hiragana_mapping[prediction]}\")\n",
    "print(f\"actual character is: {hiragana_mapping[np.argmax(onehot_test_y[i])]}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(X_test[i], cmap=\"gray\")\n",
    "plt.show()"
   ],
   "id": "3729f83870ece303",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example prediction\n",
    "i = np.random.randint(0, len(flat_test_X))\n",
    "prediction = net.predict(flat_test_X[i])\n",
    "print(f\"predicted character is: {hiragana_mapping[prediction]}\")\n",
    "print(f\"actual character is: {hiragana_mapping[np.argmax(onehot_test_y[i])]}\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(X_test[i], cmap=\"gray\")\n",
    "plt.show()"
   ],
   "id": "fdb1e4f61e060f90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note that this character is the same as the one above it, but is drastically different looking.",
   "id": "25b9a83ab8d9768e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T19:26:36.362663Z",
     "start_time": "2025-05-05T19:26:36.179832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get accuracy over all 10,000 test instances\n",
    "\n",
    "incorrect_predictions = 0\n",
    "for i in range(len(flat_test_X)):\n",
    "    prediction = net.predict(flat_test_X[i])\n",
    "    if prediction != np.argmax(onehot_test_y[i]): incorrect_predictions += 1\n",
    "\n",
    "print(f\"Incorrect predictions: {incorrect_predictions}\")\n",
    "print(f\"Classification Accuracy: {(10000 - incorrect_predictions)/10000}\")"
   ],
   "id": "dd316499a436140d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect predictions: 1447\n",
      "Classification Accuracy: 0.8553\n"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T21:01:48.665663Z",
     "start_time": "2025-05-05T21:01:48.654917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Experiment with other network architectures\n",
    "\n",
    "net2 = DenseNetwork(layers=(784, 150, 150, 10))\n",
    "net3 = DenseNetwork(layers=(784, 200, 80, 10))\n",
    "net4 = DenseNetwork(layers=(784, 50, 30, 10))\n",
    "net5 = DenseNetwork(layers=(784, 40, 40, 40, 10))\n",
    "print(len(net5.B))"
   ],
   "id": "7bdc51b8f0559620",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T22:01:36.657530Z",
     "start_time": "2025-05-05T21:30:58.168101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train network with 150 neurons per hidden layer\n",
    "\n",
    "net2.train(flat_train_X, onehot_train_y, alpha=0.05, epochs=100)"
   ],
   "id": "cc7fa935da2ef0af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Cost = [1.28233803]\n",
      "1-Epoch Cost = [0.1123868]\n",
      "2-Epoch Cost = [0.07789027]\n",
      "3-Epoch Cost = [0.06180685]\n",
      "4-Epoch Cost = [0.0508069]\n",
      "5-Epoch Cost = [0.04218064]\n",
      "6-Epoch Cost = [0.03481296]\n",
      "7-Epoch Cost = [0.03027216]\n",
      "8-Epoch Cost = [0.02757797]\n",
      "9-Epoch Cost = [0.02456634]\n",
      "10-Epoch Cost = [0.02153433]\n",
      "11-Epoch Cost = [0.01969899]\n",
      "12-Epoch Cost = [0.01802901]\n",
      "13-Epoch Cost = [0.01676226]\n",
      "14-Epoch Cost = [0.01565154]\n",
      "15-Epoch Cost = [0.0146347]\n",
      "16-Epoch Cost = [0.01380663]\n",
      "17-Epoch Cost = [0.0125669]\n",
      "18-Epoch Cost = [0.01154784]\n",
      "19-Epoch Cost = [0.0107455]\n",
      "20-Epoch Cost = [0.01005847]\n",
      "21-Epoch Cost = [0.00951585]\n",
      "22-Epoch Cost = [0.00906456]\n",
      "23-Epoch Cost = [0.00877126]\n",
      "24-Epoch Cost = [0.00818432]\n",
      "25-Epoch Cost = [0.00799328]\n",
      "26-Epoch Cost = [0.00775002]\n",
      "27-Epoch Cost = [0.00738715]\n",
      "28-Epoch Cost = [0.00711741]\n",
      "29-Epoch Cost = [0.00687577]\n",
      "30-Epoch Cost = [0.0066141]\n",
      "31-Epoch Cost = [0.00656828]\n",
      "32-Epoch Cost = [0.00631618]\n",
      "33-Epoch Cost = [0.00617905]\n",
      "34-Epoch Cost = [0.00601947]\n",
      "35-Epoch Cost = [0.0059392]\n",
      "36-Epoch Cost = [0.00589443]\n",
      "37-Epoch Cost = [0.00594947]\n",
      "38-Epoch Cost = [0.00603602]\n",
      "39-Epoch Cost = [0.00555296]\n",
      "40-Epoch Cost = [0.00545198]\n",
      "41-Epoch Cost = [0.00537055]\n",
      "42-Epoch Cost = [0.00529596]\n",
      "43-Epoch Cost = [0.00525522]\n",
      "44-Epoch Cost = [0.00519423]\n",
      "45-Epoch Cost = [0.00513306]\n",
      "46-Epoch Cost = [0.00506006]\n",
      "47-Epoch Cost = [0.00500787]\n",
      "48-Epoch Cost = [0.00504409]\n",
      "49-Epoch Cost = [0.00493366]\n",
      "50-Epoch Cost = [0.00488684]\n",
      "51-Epoch Cost = [0.00485627]\n",
      "52-Epoch Cost = [0.00482323]\n",
      "53-Epoch Cost = [0.00478173]\n",
      "54-Epoch Cost = [0.00474827]\n",
      "55-Epoch Cost = [0.00472337]\n",
      "56-Epoch Cost = [0.0047308]\n",
      "57-Epoch Cost = [0.00475786]\n",
      "58-Epoch Cost = [0.00463627]\n",
      "59-Epoch Cost = [0.00459003]\n",
      "60-Epoch Cost = [0.004561]\n",
      "61-Epoch Cost = [0.00454054]\n",
      "62-Epoch Cost = [0.00451581]\n",
      "63-Epoch Cost = [0.00453999]\n",
      "64-Epoch Cost = [0.00446358]\n",
      "65-Epoch Cost = [0.0044274]\n",
      "66-Epoch Cost = [0.00439813]\n",
      "67-Epoch Cost = [0.00438529]\n",
      "68-Epoch Cost = [0.0043609]\n",
      "69-Epoch Cost = [0.00434103]\n",
      "70-Epoch Cost = [0.00433679]\n",
      "71-Epoch Cost = [0.00429493]\n",
      "72-Epoch Cost = [0.0042789]\n",
      "73-Epoch Cost = [0.00425664]\n",
      "74-Epoch Cost = [0.00422962]\n",
      "75-Epoch Cost = [0.00420497]\n",
      "76-Epoch Cost = [0.00418671]\n",
      "77-Epoch Cost = [0.00417295]\n",
      "78-Epoch Cost = [0.0041669]\n",
      "79-Epoch Cost = [0.00414251]\n",
      "80-Epoch Cost = [0.00413491]\n",
      "81-Epoch Cost = [0.00412917]\n",
      "82-Epoch Cost = [0.00412058]\n",
      "83-Epoch Cost = [0.00410718]\n",
      "84-Epoch Cost = [0.00409317]\n",
      "85-Epoch Cost = [0.0040808]\n",
      "86-Epoch Cost = [0.00406775]\n",
      "87-Epoch Cost = [0.00406232]\n",
      "88-Epoch Cost = [0.00404325]\n",
      "89-Epoch Cost = [0.00403514]\n",
      "90-Epoch Cost = [0.00402817]\n",
      "91-Epoch Cost = [0.00401691]\n",
      "92-Epoch Cost = [0.00400281]\n",
      "93-Epoch Cost = [0.0039927]\n",
      "94-Epoch Cost = [0.00398171]\n",
      "95-Epoch Cost = [0.00397725]\n",
      "96-Epoch Cost = [0.00396224]\n",
      "97-Epoch Cost = [0.00394967]\n",
      "98-Epoch Cost = [0.00393667]\n",
      "99-Epoch Cost = [0.00392031]\n",
      "100-Epoch Cost = [0.00390529]\n"
     ]
    }
   ],
   "execution_count": 129
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T22:37:54.263708Z",
     "start_time": "2025-05-05T22:02:52.470562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train network with 200 and 80 neurons per hidden layer\n",
    "\n",
    "net3.train(flat_train_X, onehot_train_y, alpha=0.05, epochs=100)"
   ],
   "id": "ee1d13223e3b4c0c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Cost = [1.42509869]\n",
      "1-Epoch Cost = [0.11175647]\n",
      "2-Epoch Cost = [0.07679256]\n",
      "3-Epoch Cost = [0.06061729]\n",
      "4-Epoch Cost = [0.04893243]\n",
      "5-Epoch Cost = [0.03997349]\n",
      "6-Epoch Cost = [0.03364308]\n",
      "7-Epoch Cost = [0.02902415]\n",
      "8-Epoch Cost = [0.02546805]\n",
      "9-Epoch Cost = [0.02277421]\n",
      "10-Epoch Cost = [0.02057884]\n",
      "11-Epoch Cost = [0.01844811]\n",
      "12-Epoch Cost = [0.01676314]\n",
      "13-Epoch Cost = [0.01615556]\n",
      "14-Epoch Cost = [0.01487646]\n",
      "15-Epoch Cost = [0.01286772]\n",
      "16-Epoch Cost = [0.01171776]\n",
      "17-Epoch Cost = [0.01067881]\n",
      "18-Epoch Cost = [0.01000871]\n",
      "19-Epoch Cost = [0.00964509]\n",
      "20-Epoch Cost = [0.00939397]\n",
      "21-Epoch Cost = [0.00850274]\n",
      "22-Epoch Cost = [0.00803835]\n",
      "23-Epoch Cost = [0.00748769]\n",
      "24-Epoch Cost = [0.00700959]\n",
      "25-Epoch Cost = [0.00672018]\n",
      "26-Epoch Cost = [0.00629006]\n",
      "27-Epoch Cost = [0.00588358]\n",
      "28-Epoch Cost = [0.00567783]\n",
      "29-Epoch Cost = [0.00537124]\n",
      "30-Epoch Cost = [0.00519018]\n",
      "31-Epoch Cost = [0.00507072]\n",
      "32-Epoch Cost = [0.00491747]\n",
      "33-Epoch Cost = [0.00477296]\n",
      "34-Epoch Cost = [0.00466931]\n",
      "35-Epoch Cost = [0.00457383]\n",
      "36-Epoch Cost = [0.00449995]\n",
      "37-Epoch Cost = [0.00440243]\n",
      "38-Epoch Cost = [0.00428699]\n",
      "39-Epoch Cost = [0.00419077]\n",
      "40-Epoch Cost = [0.00411764]\n",
      "41-Epoch Cost = [0.00406004]\n",
      "42-Epoch Cost = [0.00399763]\n",
      "43-Epoch Cost = [0.00393044]\n",
      "44-Epoch Cost = [0.00386827]\n",
      "45-Epoch Cost = [0.00380427]\n",
      "46-Epoch Cost = [0.00372992]\n",
      "47-Epoch Cost = [0.00367064]\n",
      "48-Epoch Cost = [0.00362673]\n",
      "49-Epoch Cost = [0.00358486]\n",
      "50-Epoch Cost = [0.00355077]\n",
      "51-Epoch Cost = [0.00351588]\n",
      "52-Epoch Cost = [0.00348458]\n",
      "53-Epoch Cost = [0.00345807]\n",
      "54-Epoch Cost = [0.00343668]\n",
      "55-Epoch Cost = [0.00340684]\n",
      "56-Epoch Cost = [0.0033854]\n",
      "57-Epoch Cost = [0.00336321]\n",
      "58-Epoch Cost = [0.00334578]\n",
      "59-Epoch Cost = [0.00332979]\n",
      "60-Epoch Cost = [0.00331381]\n",
      "61-Epoch Cost = [0.00329946]\n",
      "62-Epoch Cost = [0.00328406]\n",
      "63-Epoch Cost = [0.00326798]\n",
      "64-Epoch Cost = [0.00326015]\n",
      "65-Epoch Cost = [0.00325739]\n",
      "66-Epoch Cost = [0.00323748]\n",
      "67-Epoch Cost = [0.00322447]\n",
      "68-Epoch Cost = [0.0031859]\n",
      "69-Epoch Cost = [0.0031708]\n",
      "70-Epoch Cost = [0.00314945]\n",
      "71-Epoch Cost = [0.00313467]\n",
      "72-Epoch Cost = [0.0031161]\n",
      "73-Epoch Cost = [0.00310858]\n",
      "74-Epoch Cost = [0.00310233]\n",
      "75-Epoch Cost = [0.00309367]\n",
      "76-Epoch Cost = [0.00307504]\n",
      "77-Epoch Cost = [0.00306941]\n",
      "78-Epoch Cost = [0.00305466]\n",
      "79-Epoch Cost = [0.00304636]\n",
      "80-Epoch Cost = [0.00306468]\n",
      "81-Epoch Cost = [0.00302908]\n",
      "82-Epoch Cost = [0.00302445]\n",
      "83-Epoch Cost = [0.00301616]\n",
      "84-Epoch Cost = [0.00300378]\n",
      "85-Epoch Cost = [0.00298976]\n",
      "86-Epoch Cost = [0.00296711]\n",
      "87-Epoch Cost = [0.00295381]\n",
      "88-Epoch Cost = [0.00293679]\n",
      "89-Epoch Cost = [0.00292424]\n",
      "90-Epoch Cost = [0.00291013]\n",
      "91-Epoch Cost = [0.00288138]\n",
      "92-Epoch Cost = [0.00286173]\n",
      "93-Epoch Cost = [0.00284622]\n",
      "94-Epoch Cost = [0.00283719]\n",
      "95-Epoch Cost = [0.00283328]\n",
      "96-Epoch Cost = [0.00282951]\n",
      "97-Epoch Cost = [0.00282644]\n",
      "98-Epoch Cost = [0.00282237]\n",
      "99-Epoch Cost = [0.00281204]\n",
      "100-Epoch Cost = [0.00280721]\n"
     ]
    }
   ],
   "execution_count": 131
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T22:52:07.938576Z",
     "start_time": "2025-05-05T22:41:04.884681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train network with 50 and 30 neurons per hidden layer\n",
    "\n",
    "net4.train(flat_train_X, onehot_train_y, alpha=0.05, epochs=100)"
   ],
   "id": "9f8f8393e293c14",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Cost = [1.41254593]\n",
      "1-Epoch Cost = [0.10521409]\n",
      "2-Epoch Cost = [0.07337457]\n",
      "3-Epoch Cost = [0.06005367]\n",
      "4-Epoch Cost = [0.05321695]\n",
      "5-Epoch Cost = [0.04831384]\n",
      "6-Epoch Cost = [0.04439007]\n",
      "7-Epoch Cost = [0.04138753]\n",
      "8-Epoch Cost = [0.03911253]\n",
      "9-Epoch Cost = [0.03726901]\n",
      "10-Epoch Cost = [0.0355193]\n",
      "11-Epoch Cost = [0.034002]\n",
      "12-Epoch Cost = [0.03282732]\n",
      "13-Epoch Cost = [0.03163716]\n",
      "14-Epoch Cost = [0.03074068]\n",
      "15-Epoch Cost = [0.03019148]\n",
      "16-Epoch Cost = [0.02975717]\n",
      "17-Epoch Cost = [0.02916786]\n",
      "18-Epoch Cost = [0.02821784]\n",
      "19-Epoch Cost = [0.02737266]\n",
      "20-Epoch Cost = [0.02657326]\n",
      "21-Epoch Cost = [0.02576058]\n",
      "22-Epoch Cost = [0.02522432]\n",
      "23-Epoch Cost = [0.02440259]\n",
      "24-Epoch Cost = [0.02346646]\n",
      "25-Epoch Cost = [0.02313561]\n",
      "26-Epoch Cost = [0.02247932]\n",
      "27-Epoch Cost = [0.02189705]\n",
      "28-Epoch Cost = [0.02180259]\n",
      "29-Epoch Cost = [0.0209316]\n",
      "30-Epoch Cost = [0.02072695]\n",
      "31-Epoch Cost = [0.02076406]\n",
      "32-Epoch Cost = [0.01981665]\n",
      "33-Epoch Cost = [0.01978463]\n",
      "34-Epoch Cost = [0.01907146]\n",
      "35-Epoch Cost = [0.01860542]\n",
      "36-Epoch Cost = [0.01766201]\n",
      "37-Epoch Cost = [0.01762993]\n",
      "38-Epoch Cost = [0.01648958]\n",
      "39-Epoch Cost = [0.01688834]\n",
      "40-Epoch Cost = [0.01621899]\n",
      "41-Epoch Cost = [0.0155544]\n",
      "42-Epoch Cost = [0.01548632]\n",
      "43-Epoch Cost = [0.01529236]\n",
      "44-Epoch Cost = [0.01515509]\n",
      "45-Epoch Cost = [0.01532142]\n",
      "46-Epoch Cost = [0.01439993]\n",
      "47-Epoch Cost = [0.01410528]\n",
      "48-Epoch Cost = [0.01393727]\n",
      "49-Epoch Cost = [0.01389003]\n",
      "50-Epoch Cost = [0.01375053]\n",
      "51-Epoch Cost = [0.01368075]\n",
      "52-Epoch Cost = [0.01341894]\n",
      "53-Epoch Cost = [0.01316557]\n",
      "54-Epoch Cost = [0.01315443]\n",
      "55-Epoch Cost = [0.01300167]\n",
      "56-Epoch Cost = [0.01311067]\n",
      "57-Epoch Cost = [0.01274108]\n",
      "58-Epoch Cost = [0.01273717]\n",
      "59-Epoch Cost = [0.01261189]\n",
      "60-Epoch Cost = [0.01241906]\n",
      "61-Epoch Cost = [0.01227371]\n",
      "62-Epoch Cost = [0.01214797]\n",
      "63-Epoch Cost = [0.01210379]\n",
      "64-Epoch Cost = [0.01199549]\n",
      "65-Epoch Cost = [0.01192976]\n",
      "66-Epoch Cost = [0.01183521]\n",
      "67-Epoch Cost = [0.01169422]\n",
      "68-Epoch Cost = [0.01163267]\n",
      "69-Epoch Cost = [0.01155142]\n",
      "70-Epoch Cost = [0.01145717]\n",
      "71-Epoch Cost = [0.01136194]\n",
      "72-Epoch Cost = [0.01129153]\n",
      "73-Epoch Cost = [0.01125285]\n",
      "74-Epoch Cost = [0.0111593]\n",
      "75-Epoch Cost = [0.01111184]\n",
      "76-Epoch Cost = [0.01107485]\n",
      "77-Epoch Cost = [0.01102633]\n",
      "78-Epoch Cost = [0.01114304]\n",
      "79-Epoch Cost = [0.01093753]\n",
      "80-Epoch Cost = [0.0108764]\n",
      "81-Epoch Cost = [0.01085294]\n",
      "82-Epoch Cost = [0.01080068]\n",
      "83-Epoch Cost = [0.01128882]\n",
      "84-Epoch Cost = [0.01215634]\n",
      "85-Epoch Cost = [0.01521175]\n",
      "86-Epoch Cost = [0.01554055]\n",
      "87-Epoch Cost = [0.01379545]\n",
      "88-Epoch Cost = [0.01454683]\n",
      "89-Epoch Cost = [0.01419661]\n",
      "90-Epoch Cost = [0.01257798]\n",
      "91-Epoch Cost = [0.01179945]\n",
      "92-Epoch Cost = [0.01128415]\n",
      "93-Epoch Cost = [0.01113974]\n",
      "94-Epoch Cost = [0.01070908]\n",
      "95-Epoch Cost = [0.01045985]\n",
      "96-Epoch Cost = [0.01037299]\n",
      "97-Epoch Cost = [0.01031533]\n",
      "98-Epoch Cost = [0.01028588]\n",
      "99-Epoch Cost = [0.01023349]\n",
      "100-Epoch Cost = [0.01020294]\n"
     ]
    }
   ],
   "execution_count": 133
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T21:17:42.926414Z",
     "start_time": "2025-05-05T21:05:45.194752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train network with 3 hidden layers, each with 40 neurons\n",
    "\n",
    "net5.train(flat_train_X, onehot_train_y, alpha=0.01, epochs=100)"
   ],
   "id": "c722d4e38b0f4eec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Cost = [0.03525019]\n",
      "1-Epoch Cost = [0.03067678]\n",
      "2-Epoch Cost = [0.02951348]\n",
      "3-Epoch Cost = [0.02881298]\n",
      "4-Epoch Cost = [0.02821254]\n",
      "5-Epoch Cost = [0.0277073]\n",
      "6-Epoch Cost = [0.02726634]\n",
      "7-Epoch Cost = [0.02688086]\n",
      "8-Epoch Cost = [0.02652622]\n",
      "9-Epoch Cost = [0.02618427]\n",
      "10-Epoch Cost = [0.02585332]\n",
      "11-Epoch Cost = [0.0255279]\n",
      "12-Epoch Cost = [0.02520677]\n",
      "13-Epoch Cost = [0.02489886]\n",
      "14-Epoch Cost = [0.02461104]\n",
      "15-Epoch Cost = [0.02435306]\n",
      "16-Epoch Cost = [0.02411552]\n",
      "17-Epoch Cost = [0.02389631]\n",
      "18-Epoch Cost = [0.02369039]\n",
      "19-Epoch Cost = [0.02349493]\n",
      "20-Epoch Cost = [0.02328056]\n",
      "21-Epoch Cost = [0.02302869]\n",
      "22-Epoch Cost = [0.02280956]\n",
      "23-Epoch Cost = [0.02268491]\n",
      "24-Epoch Cost = [0.02258431]\n",
      "25-Epoch Cost = [0.02242235]\n",
      "26-Epoch Cost = [0.02223978]\n",
      "27-Epoch Cost = [0.02201442]\n",
      "28-Epoch Cost = [0.02176793]\n",
      "29-Epoch Cost = [0.02154439]\n",
      "30-Epoch Cost = [0.02133884]\n",
      "31-Epoch Cost = [0.02114612]\n",
      "32-Epoch Cost = [0.02096422]\n",
      "33-Epoch Cost = [0.02079712]\n",
      "34-Epoch Cost = [0.02064677]\n",
      "35-Epoch Cost = [0.02050853]\n",
      "36-Epoch Cost = [0.02036841]\n",
      "37-Epoch Cost = [0.02023047]\n",
      "38-Epoch Cost = [0.02010109]\n",
      "39-Epoch Cost = [0.01997825]\n",
      "40-Epoch Cost = [0.01985551]\n",
      "41-Epoch Cost = [0.01973965]\n",
      "42-Epoch Cost = [0.01964489]\n",
      "43-Epoch Cost = [0.01953752]\n",
      "44-Epoch Cost = [0.0194435]\n",
      "45-Epoch Cost = [0.01935905]\n",
      "46-Epoch Cost = [0.01922264]\n",
      "47-Epoch Cost = [0.01909608]\n",
      "48-Epoch Cost = [0.01898735]\n",
      "49-Epoch Cost = [0.01888157]\n",
      "50-Epoch Cost = [0.01878182]\n",
      "51-Epoch Cost = [0.01868864]\n",
      "52-Epoch Cost = [0.01859363]\n",
      "53-Epoch Cost = [0.01849834]\n",
      "54-Epoch Cost = [0.01840556]\n",
      "55-Epoch Cost = [0.01831734]\n",
      "56-Epoch Cost = [0.01823186]\n",
      "57-Epoch Cost = [0.01815753]\n",
      "58-Epoch Cost = [0.01807791]\n",
      "59-Epoch Cost = [0.01800622]\n",
      "60-Epoch Cost = [0.01793356]\n",
      "61-Epoch Cost = [0.01786342]\n",
      "62-Epoch Cost = [0.01780367]\n",
      "63-Epoch Cost = [0.01776089]\n",
      "64-Epoch Cost = [0.01771754]\n",
      "65-Epoch Cost = [0.01761234]\n",
      "66-Epoch Cost = [0.01753832]\n",
      "67-Epoch Cost = [0.01746611]\n",
      "68-Epoch Cost = [0.01739663]\n",
      "69-Epoch Cost = [0.01732864]\n",
      "70-Epoch Cost = [0.01726665]\n",
      "71-Epoch Cost = [0.0172037]\n",
      "72-Epoch Cost = [0.01714946]\n",
      "73-Epoch Cost = [0.01708472]\n",
      "74-Epoch Cost = [0.01703107]\n",
      "75-Epoch Cost = [0.01696195]\n",
      "76-Epoch Cost = [0.01690623]\n",
      "77-Epoch Cost = [0.01683361]\n",
      "78-Epoch Cost = [0.01677745]\n",
      "79-Epoch Cost = [0.01670651]\n",
      "80-Epoch Cost = [0.01666029]\n",
      "81-Epoch Cost = [0.01659564]\n",
      "82-Epoch Cost = [0.01655161]\n",
      "83-Epoch Cost = [0.01649042]\n",
      "84-Epoch Cost = [0.01645607]\n",
      "85-Epoch Cost = [0.01639952]\n",
      "86-Epoch Cost = [0.01636596]\n",
      "87-Epoch Cost = [0.01631047]\n",
      "88-Epoch Cost = [0.0162857]\n",
      "89-Epoch Cost = [0.01622331]\n",
      "90-Epoch Cost = [0.01618866]\n",
      "91-Epoch Cost = [0.01612281]\n",
      "92-Epoch Cost = [0.0160952]\n",
      "93-Epoch Cost = [0.0160385]\n",
      "94-Epoch Cost = [0.01601001]\n",
      "95-Epoch Cost = [0.01595677]\n",
      "96-Epoch Cost = [0.01592877]\n",
      "97-Epoch Cost = [0.01588018]\n",
      "98-Epoch Cost = [0.0158515]\n",
      "99-Epoch Cost = [0.01581086]\n",
      "100-Epoch Cost = [0.01578537]\n"
     ]
    }
   ],
   "execution_count": 121
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T22:52:15.922723Z",
     "start_time": "2025-05-05T22:52:15.022970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get accuracy over all 10,000 test instances for alternate architectures\n",
    "\n",
    "incorrect_predictions_net2 = 0\n",
    "incorrect_predictions_net3 = 0\n",
    "incorrect_predictions_net4 = 0\n",
    "incorrect_predictions_net5 = 0\n",
    "for i in range(len(flat_test_X)):\n",
    "    prediction_net2 = net2.predict(flat_test_X[i])\n",
    "    if prediction_net2 != np.argmax(onehot_test_y[i]): incorrect_predictions_net2 += 1\n",
    "    prediction_net3 = net3.predict(flat_test_X[i])\n",
    "    if prediction_net3 != np.argmax(onehot_test_y[i]): incorrect_predictions_net3 += 1\n",
    "    prediction_net4 = net4.predict(flat_test_X[i])\n",
    "    if prediction_net4 != np.argmax(onehot_test_y[i]): incorrect_predictions_net4 += 1\n",
    "    prediction_net5 = net5.predict(flat_test_X[i])\n",
    "    if prediction_net5 != np.argmax(onehot_test_y[i]): incorrect_predictions_net5 += 1\n",
    "\n",
    "print(f\"Network 1 Classification Accuracy: {(10000 - incorrect_predictions)/10000}\")\n",
    "print(f\"Network 2 Classification Accuracy: {(10000 - incorrect_predictions_net2)/10000}\")\n",
    "print(f\"Network 3 Classification Accuracy: {(10000 - incorrect_predictions_net3)/10000}\")\n",
    "print(f\"Network 4 Classification Accuracy: {(10000 - incorrect_predictions_net4)/10000}\")\n",
    "print(f\"Network 5 Classification Accuracy: {(10000 - incorrect_predictions_net5)/10000}\")"
   ],
   "id": "8e70d9b34fe86904",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network 1 Classification Accuracy: 0.8553\n",
      "Network 2 Classification Accuracy: 0.8951\n",
      "Network 3 Classification Accuracy: 0.9067\n",
      "Network 4 Classification Accuracy: 0.8395\n",
      "Network 5 Classification Accuracy: 0.8194\n"
     ]
    }
   ],
   "execution_count": 134
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Conclusion:\n",
    "With the neural network architecture of two hidden layers, having 60 neurons each, the model is able to correctly classify the Japanese handwritten characters about 85.5% of the time. This is after 100 epochs of training, which took about 15 minutes to run. This is a relatively good result, considering the wide variety of ways that the same character is drawn, and the low 28x28 pixel resolution. I tested 4 additional network architectures, with hidden layers schemes of (150, 150), (200, 80), (50, 30), and (40, 40, 40). All of these ran for 100 epochs and 0.05 learning rate, except for the (40, 40, 40) one, which needed a smaller step size, so I used 0.01. The two things to note are the classification accuracy and the runtime. the architecture of (200, 80) had the highest accuracy, but took 35 minutes to complete 100 epochs of training. Similarly, the (150, 150) structure took 30 minutes and had almost as good of an accuracy. The (50, 30) and the (40, 40, 40) structures were less accurate than the (60, 60) model, and ran for similar amounts of time. With 784 features, it seems that it is more accurate to use as many nodes as possible, and likely no more than two layers is useful in this case. At some point, increasing the number of neurons may begin to overfit the training data, resulting in decreased accuracy on the testing set despite a very low MSE on the training set. However, that point was not yet reached in this case. Additionally, each structure may do better with a more fine-tuned learning rate."
   ],
   "id": "8403975f33a9dc4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
